{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7b379f2",
   "metadata": {},
   "source": [
    "Read dataframe of 04/2023-04/2024 news headlines related to several stocks (AMD, AAPL, NFLX, AMZN, TSCO, WMT, MSFT, LLY, GOOG, NKE, TSLA, QQQ, NVDA, stock market).\n",
    "The data was scrapped with my own code and by sending requests to finnhub.io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e679445",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, inspect\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9c65cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine('sqlite:///../output/AAPL_2023-04-06_2024-04-06/financial_data.db', echo=False)\n",
    "df = pd.read_sql('AAPL_', con=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a829a6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf02167",
   "metadata": {},
   "source": [
    "The next step before adding the labels to the data is to calculate the emotions of the headlines using FinBERT pre-trained textual model.\n",
    "\n",
    "We are going to use a pipeline that already does for us all the preprocessing and tokenizing of the textual data so we only need to send the headlines as inputs and get the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0f41a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from sqlalchemy import MetaData, inspect\n",
    "\n",
    "def batch(data, batch_size):\n",
    "    \"\"\"Batch the data to avoid too high memory consumption\"\"\"\n",
    "    batches = []\n",
    "    \n",
    "    for i in range(0, len(data), batch_size):\n",
    "        batches.append(data[i:i + batch_size])\n",
    "    return batches\n",
    "headlines_list = list(df.headline)\n",
    "headlines_batches = batch(headlines_list, 100)\n",
    "\n",
    "pipe = pipeline(\"text-classification\", model=\"ProsusAI/finbert\", return_all_scores=True)\n",
    "outputs = list()\n",
    "\n",
    "for index, batch in enumerate(headlines_batches):\n",
    "    print(f\"index: {index*100}\", end=\"\\r\")\n",
    "    output_polarity = pipe(batch)\n",
    "    outputs += output_polarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64620c4b",
   "metadata": {},
   "source": [
    "After adding the processed data the df will look like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6aa4683f",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine('sqlite:///../output/output_with_emotions/AAPL_2023-04-06_2024-04-06.db', echo=False)\n",
    "df = pd.read_sql('AAPL_', con=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "41411366",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>category</th>\n",
       "      <th>datetime</th>\n",
       "      <th>headline</th>\n",
       "      <th>id</th>\n",
       "      <th>image</th>\n",
       "      <th>related</th>\n",
       "      <th>source</th>\n",
       "      <th>summary</th>\n",
       "      <th>url</th>\n",
       "      <th>positive</th>\n",
       "      <th>negative</th>\n",
       "      <th>neutral</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>company</td>\n",
       "      <td>1681324461</td>\n",
       "      <td>Bitcoin Surges 81% YTD: Is The Rally For Real ...</td>\n",
       "      <td>119733697</td>\n",
       "      <td>https://static.seekingalpha.com/cdn/s3/uploads...</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>SeekingAlpha</td>\n",
       "      <td>Bitcoin is skyrocketing, now up roughly 81% YT...</td>\n",
       "      <td>https://finnhub.io/api/news?id=02a1949786f4080...</td>\n",
       "      <td>0.091022</td>\n",
       "      <td>0.076031</td>\n",
       "      <td>0.832947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>company</td>\n",
       "      <td>1681318105</td>\n",
       "      <td>UPDATE 1-Cirrus Logic slumps as analyst says A...</td>\n",
       "      <td>119729225</td>\n",
       "      <td></td>\n",
       "      <td>AAPL</td>\n",
       "      <td>Reuters</td>\n",
       "      <td>Shares of Apple Inc supplier Cirrus Logic Inc ...</td>\n",
       "      <td>https://finnhub.io/api/news?id=c22e367f55d9aa1...</td>\n",
       "      <td>0.010884</td>\n",
       "      <td>0.970709</td>\n",
       "      <td>0.018407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>company</td>\n",
       "      <td>1681316532</td>\n",
       "      <td>Cirrus Logic slumps as analyst says Apple to a...</td>\n",
       "      <td>119729226</td>\n",
       "      <td></td>\n",
       "      <td>AAPL</td>\n",
       "      <td>Reuters</td>\n",
       "      <td>Shares of Apple Inc supplier Cirrus Logic Inc ...</td>\n",
       "      <td>https://finnhub.io/api/news?id=eab0fcff4d4cb98...</td>\n",
       "      <td>0.011214</td>\n",
       "      <td>0.964231</td>\n",
       "      <td>0.024555</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index category    datetime  \\\n",
       "0      0  company  1681324461   \n",
       "1      1  company  1681318105   \n",
       "2      2  company  1681316532   \n",
       "\n",
       "                                            headline         id  \\\n",
       "0  Bitcoin Surges 81% YTD: Is The Rally For Real ...  119733697   \n",
       "1  UPDATE 1-Cirrus Logic slumps as analyst says A...  119729225   \n",
       "2  Cirrus Logic slumps as analyst says Apple to a...  119729226   \n",
       "\n",
       "                                               image related        source  \\\n",
       "0  https://static.seekingalpha.com/cdn/s3/uploads...    AAPL  SeekingAlpha   \n",
       "1                                                       AAPL       Reuters   \n",
       "2                                                       AAPL       Reuters   \n",
       "\n",
       "                                             summary  \\\n",
       "0  Bitcoin is skyrocketing, now up roughly 81% YT...   \n",
       "1  Shares of Apple Inc supplier Cirrus Logic Inc ...   \n",
       "2  Shares of Apple Inc supplier Cirrus Logic Inc ...   \n",
       "\n",
       "                                                 url  positive  negative  \\\n",
       "0  https://finnhub.io/api/news?id=02a1949786f4080...  0.091022  0.076031   \n",
       "1  https://finnhub.io/api/news?id=c22e367f55d9aa1...  0.010884  0.970709   \n",
       "2  https://finnhub.io/api/news?id=eab0fcff4d4cb98...  0.011214  0.964231   \n",
       "\n",
       "    neutral  \n",
       "0  0.832947  \n",
       "1  0.018407  \n",
       "2  0.024555  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955b45b0",
   "metadata": {},
   "source": [
    "Add labels to the data using yfinance and each time calculating if the price of the relevant stock went up the next hour or went down\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f75b2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_str_dates(df):\n",
    "    dates_list = list()\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        # Convert date of headline to str dates \n",
    "        date_obj = datetime.utcfromtimestamp(row.datetime)\n",
    "\n",
    "        str_date = date_obj.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        dates_list.append(str_date)\n",
    "    df[\"str_date\"] = dates_list\n",
    "    \n",
    "\n",
    "def add_market_change(data):\n",
    "    data_change = [-1]\n",
    "\n",
    "    for num_index, (index, row) in enumerate(data[1:].iterrows()):\n",
    "\n",
    "        if row.Open > data.iloc[num_index].Close:\n",
    "            data_change.append(1)\n",
    "        else:\n",
    "            data_change.append(0)\n",
    "\n",
    "    data['data_change'] = data_change\n",
    "        \n",
    "\n",
    "def find_next_open_date(data, date, counter=0):\n",
    "    \n",
    "    if counter == 10:\n",
    "        return -1\n",
    "    \n",
    "    date_obj = datetime.strptime(date, \"%Y-%m-%d %H:%M:%S\")\n",
    "    date_obj += timedelta(days=1)\n",
    "    date_str = date_obj.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    \n",
    "    if date_str[:10] not in data.index:\n",
    "        return find_next_open_date(data, date_str, counter+1)\n",
    "    return data.loc[date_str[:10]].data_change\n",
    "        \n",
    "\n",
    "def get_finance_data(data, df):\n",
    "    finance_data = list()\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        finance_data_obj = find_next_open_date(data, row.str_date, counter=0)\n",
    "        finance_data.append(int(finance_data_obj))\n",
    "    df[\"market_change\"] = finance_data\n",
    "    \n",
    "\n",
    "news_dir = \"../output/output_with_emotions\"\n",
    "news_with_finance_and_emotions = \"../output/output_with_emotions_and_finance\"\n",
    "\n",
    "if not os.path.exists(news_with_finance_and_emotions):\n",
    "    os.mkdir(news_with_finance_and_emotions)\n",
    "\n",
    "\n",
    "for file in os.listdir(news_dir):\n",
    "    \n",
    "    ticker = file.split(\"_\")[0]\n",
    "    \n",
    "    engine = create_engine(f\"sqlite:///{news_dir}/{file}\", echo=False)\n",
    "    df = pd.read_sql(f\"{ticker}_\", con=engine, index_col=\"index\")\n",
    "        \n",
    "    data = yf.download(ticker, period=\"1y\")\n",
    "    \n",
    "    get_str_dates(df)\n",
    "    add_market_change(data)\n",
    "    get_finance_data(data, df)\n",
    "        \n",
    "    df.drop(columns=['url', 'image', 'id', 'headline', 'datetime', 'summary', 'category'], inplace=True)\n",
    "    engine_output = create_engine(f\"sqlite:///{news_with_finance_and_emotions}/{file}\", echo=False)\n",
    "    df.to_sql(f\"{ticker}_\", con=engine_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e260c9f",
   "metadata": {},
   "source": [
    "Some of the news headlines are just adds so we need to remove them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8ece0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_adds(df):\n",
    "    indexes_to_remove = list()\n",
    "\n",
    "    start_date = datetime.strptime(\"2023-04-12\", \"%Y-%m-%d\")\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        date_obj = datetime.strptime(row.str_date, \"%Y-%m-%d %H:%M:%S\")\n",
    "        if date_obj < start_date:\n",
    "            indexes_to_remove.append(index)\n",
    "        else:\n",
    "            break\n",
    "    df.drop(indexes_to_remove, inplace=True)\n",
    "    \n",
    "    \n",
    "# Combining all the data\n",
    "df = pd.DataFrame(columns=[\"related\", \"source\", \"positive\", \n",
    "                           \"negative\", \"neutral\", \"str_date\", \"market_change\"])\n",
    "\n",
    "for file in os.listdir(news_with_finance_and_emotions):\n",
    "    \n",
    "    if file == \".DS_Store\":\n",
    "        continue\n",
    "    ticker = file.split(\"_\")[0]\n",
    "    engine = create_engine(f\"sqlite:///{news_with_finance_and_emotions}/{file}\", echo=False)\n",
    "    \n",
    "    df_tmp = pd.read_sql(f\"{ticker}_\", con=engine, index_col=\"index\")\n",
    "    df = pd.concat([df, df_tmp])\n",
    "\n",
    "# Convert text (that represent a category) to numbers\n",
    "for col in ['source']:\n",
    "    df[col], _ = pd.factorize(df[col])\n",
    "df.sort_values(\"str_date\", inplace=True)\n",
    "df.reset_index(inplace=True, drop=True)\n",
    "remove_adds(df)\n",
    "df.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b7a174",
   "metadata": {},
   "source": [
    "The final results -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3fb7f9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine('sqlite:///../output/all_output_parsed_hourly_market_change.db', echo=False)\n",
    "df = pd.read_sql('ALL', con=engine)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ca910870",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>related</th>\n",
       "      <th>source</th>\n",
       "      <th>positive</th>\n",
       "      <th>negative</th>\n",
       "      <th>neutral</th>\n",
       "      <th>str_date</th>\n",
       "      <th>market_change</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>118279</th>\n",
       "      <td>118279</td>\n",
       "      <td>QQQ</td>\n",
       "      <td>0</td>\n",
       "      <td>0.359381</td>\n",
       "      <td>0.612389</td>\n",
       "      <td>0.028230</td>\n",
       "      <td>2024-04-06 03:30:00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118280</th>\n",
       "      <td>118280</td>\n",
       "      <td>QQQ</td>\n",
       "      <td>0</td>\n",
       "      <td>0.072482</td>\n",
       "      <td>0.879065</td>\n",
       "      <td>0.048453</td>\n",
       "      <td>2024-04-06 03:50:00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118281</th>\n",
       "      <td>118281</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>0</td>\n",
       "      <td>0.026854</td>\n",
       "      <td>0.027760</td>\n",
       "      <td>0.945386</td>\n",
       "      <td>2024-04-06 06:45:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         index related  source  positive  negative   neutral  \\\n",
       "118279  118279     QQQ       0  0.359381  0.612389  0.028230   \n",
       "118280  118280     QQQ       0  0.072482  0.879065  0.048453   \n",
       "118281  118281    AAPL       0  0.026854  0.027760  0.945386   \n",
       "\n",
       "                   str_date  market_change  \n",
       "118279  2024-04-06 03:30:00              1  \n",
       "118280  2024-04-06 03:50:00              1  \n",
       "118281  2024-04-06 06:45:00              0  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100437a4",
   "metadata": {},
   "source": [
    "related is the stock, source is the website from which the headline came (transformed to integer categories) and the market_change is the label that we want to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2bc72b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(df[df.market_change == -1].index, inplace=True)\n",
    "for col in ['related']:\n",
    "    df[col], _ = pd.factorize(df[col])\n",
    "df.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c01bbc",
   "metadata": {},
   "source": [
    "## Known models verification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0e5af2",
   "metadata": {},
   "source": [
    "Prepare the DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e8b8b84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(df[df.market_change==-1].index, inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "label = df.pop(\"market_change\")\n",
    "\n",
    "# Convert 'related' to an integer represantation of category \n",
    "# (in a later stage i will convert it to a one-hot array) \n",
    "for col in ['related']:\n",
    "    df[col], _ = pd.factorize(df[col])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59cc0930",
   "metadata": {},
   "source": [
    "Split the data to test and train datasets by shuffling the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e8f01a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df.drop(columns=[\"index\", \"str_date\"], inplace=True)\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(df, label)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c48c81f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5550a5c0",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd07a44d",
   "metadata": {},
   "source": [
    "C - regularization parameter\n",
    "\n",
    "class-weights - Used when a certain class is missrrepresented in the data so we give this class higher weights so the model can learn from the few examples it has faster (in this case the data is well balanced)\n",
    "\n",
    "degree - The polinomial degree assigned to that task\n",
    "\n",
    "gamma -  used to control the influence of distant training points in the rbf kernel (i am using the default value of gamma parameter which scales gamma based on the number of features)\n",
    "\n",
    "kernel - describes the kernel function used to calculate the similarities between the functions and i used the default param\n",
    "\n",
    "probability - indicates the model to output the probability of being part of a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "26527ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_clf = SVC(C=200, class_weight=None, degree=2, gamma=\"scale\", kernel=\"rbf\", probability=True)\n",
    "svm_clf.fit(X_train_full, y_train_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f3cf29",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = svm_clf.predict_proba(X_test)\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39790e48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4fb298d2",
   "metadata": {},
   "source": [
    "## Random forest CLF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb9bab5",
   "metadata": {},
   "source": [
    "n_estimators - how many \"trees\" are in the forest\n",
    "\n",
    "max_leaf_nodes - how many leafs can each tree have (higher number of leafs is a more complex model which can also overfit)\n",
    "\n",
    "n_jobs - how many threads to run in order to train the model (-1 means everything available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ff2a1c13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5302054376567903\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rnd_clf = RandomForestClassifier(n_estimators=1000, max_leaf_nodes=64, n_jobs=-1)\n",
    "rnd_clf.fit(X_train_full, y_train_full)\n",
    "\n",
    "y_pred_rf = rnd_clf.predict(X_test)\n",
    "\n",
    "print(accuracy_score(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc6716f",
   "metadata": {},
   "source": [
    "Note that if i use predict_proba function i can decide to use only prediction with a certain level of certainty which improves the model to get an accuracy of approximately 0.59%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "48163dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6076 7933]\n",
      " [6052 9437]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5c4c1162",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "market_change\n",
       "1    15489\n",
       "0    14009\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1f5471",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7918f04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa7d894",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "55905a17",
   "metadata": {},
   "source": [
    "## Gradient boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4aa5f85",
   "metadata": {},
   "source": [
    "max_depth - The max depth for each decision tree\n",
    "\n",
    "n_estimators - How many trees in the ensamble\n",
    "\n",
    "learning_rate - Used to tweek the size of each step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8476fe5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5259000610210862\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=3, learning_rate=1.0)\n",
    "gbrt.fit(X_train_full, y_train_full)\n",
    "\n",
    "y_pred = gbrt.predict(X_test)\n",
    "y_pred = np.round(y_pred).astype(int)\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a72504",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448b3a26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c21f0bce",
   "metadata": {},
   "source": [
    "## Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9630584b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_clf = LogisticRegression()\n",
    "rnd_clf = RandomForestClassifier()\n",
    "svm_clf = SVC()\n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],\n",
    "    voting='hard')\n",
    "voting_clf.fit(X_train_full, y_train_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18872c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = voting_clf.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1af294",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "983e1b17",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18aabf78",
   "metadata": {},
   "source": [
    "Trying to train LSTM model with the data to give it some context of the previous article as they can affect the accuracy of the current news headline.\n",
    "\n",
    "Note - The data is currently a mix of all the stocks which can affect the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "50fb8df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_data(df, seq_length_news=32):\n",
    "    '''\n",
    "    Reshaping the data to be in batches form (each input value will be now an array of <seq_length_news>\n",
    "    rows from the dataframe)\n",
    "    \n",
    "    return value - \n",
    "    validation, training and testing data in the correct shape fot LSTM model\n",
    "    '''\n",
    "    # Assuming your DataFrame is already loaded, let's split it into features (X) and labels (y)\n",
    "    X = df[['related', 'source', 'positive', 'negative', 'neutral']].values  # Assuming 'related' and 'source' are your features\n",
    "    y = df[[\"market_change\"]].values  # Assuming 'positive', 'negative', 'neutral' are your labels\n",
    "\n",
    "    # Reshape the data for LSTM input (assuming your data has a time dimension)\n",
    "    n_features = X.shape[1]  # Number of features\n",
    "\n",
    "    # Split the data into train and test sets while preserving the sequence\n",
    "    test_size = 0.2\n",
    "    val_size = 0.1\n",
    "    split_index = int((1 - test_size) * len(df))\n",
    "    split_val = int(split_index*0.9)\n",
    "\n",
    "    X_train_full, X_test = X[:split_index], X[split_index:]\n",
    "    y_train_full, y_test = y[:split_index], y[split_index:]\n",
    "    X_train, X_val =  X_train_full[:split_val], X_train_full[split_val:]\n",
    "    y_train, y_val =  y_train_full[:split_val], y_train_full[split_val:]\n",
    "\n",
    "    # Train reshape\n",
    "    n_samples_train = len(X_train) - seq_length_news + 1\n",
    "    X_train_reshaped = np.zeros((n_samples_train, seq_length_news, n_features))\n",
    "    y_train_reshaped = np.zeros(n_samples_train)\n",
    "    for i in range(n_samples_train):\n",
    "        X_train_reshaped[i] = X_train[i:i + seq_length_news]\n",
    "        y_train_reshaped[i] = y_train[i + seq_length_news-1]\n",
    "\n",
    "    # Val reshape\n",
    "    n_samples_train = len(X_val) - seq_length_news + 1\n",
    "    X_val_reshaped = np.zeros((n_samples_train, seq_length_news, n_features))\n",
    "    y_val_reshaped = np.zeros(n_samples_train)\n",
    "    for i in range(n_samples_train):\n",
    "        X_val_reshaped[i] = X_val[i:i + seq_length_news]\n",
    "        y_val_reshaped[i] = y_val[i + seq_length_news-1]\n",
    "\n",
    "    n_samples_train = len(X_test) - seq_length_news + 1\n",
    "    X_test_reshaped = np.zeros((n_samples_train, seq_length_news, n_features))\n",
    "    y_test_reshaped = np.zeros(n_samples_train)\n",
    "    for i in range(n_samples_train):\n",
    "        X_test_reshaped[i] = X_test[i:i + seq_length_news]\n",
    "        y_test_reshaped[i] = y_test[i + seq_length_news-1]\n",
    "    return X_train_reshaped, y_train_reshaped, X_val_reshaped, y_val_reshaped, X_test_reshaped, y_test_reshaped"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a757cffb",
   "metadata": {},
   "source": [
    "Defining callbacks to avoid overfitting, and to save the best model at each iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7f5bd398",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "# Define the checkpoint filepath\n",
    "filepath = \"/content/drive/MyDrive/model_checkpoints_LSTM/checkpoint-{epoch:02d}.keras\"\n",
    "\n",
    "# Define the callback\n",
    "checkpoint = ModelCheckpoint(\n",
    "    filepath=filepath,\n",
    "    monitor=\"val_loss\",\n",
    "    save_best_only=True,\n",
    "    save_weights_only=False,\n",
    ")\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=4, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "22b1b326",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "X = df[['related', 'source', 'positive', 'negative', 'neutral']].values  \n",
    "y = df[[\"market_change\"]].values\n",
    "\n",
    "# Reshape the data for LSTM input\n",
    "n_features = X.shape[1]  # Number of features\n",
    "seq_length_news = 32\n",
    "\n",
    "X_train_reshaped, y_train_reshaped, X_val_reshaped, y_val_reshaped, X_test_reshaped, y_test_reshaped = \\\n",
    "reshape_data(df, seq_length_news=seq_length_news)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "717b085e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53.8791460160122\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.initializers import HeNormal\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, activation='relu', input_shape=(seq_length_news, n_features), kernel_initializer=HeNormal()))\n",
    "model.add(Dense(32, activation=\"relu\", kernel_initializer=HeNormal()))\n",
    "model.add(Dense(1, activation=\"sigmoid\", kernel_initializer=HeNormal()))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train_reshaped, y_train_reshaped, epochs=50,\n",
    "                  batch_size=seq_length_news,\n",
    "                  validation_data=(X_val_reshaped, y_val_reshaped),\n",
    "                  callbacks=[checkpoint, early_stopping])\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred_rf = rnd_clf.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87fc0de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ff2131af",
   "metadata": {},
   "source": [
    "### Search grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7451fe57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "\n",
    "\n",
    "def create_model(neurons, dropout_rate, learning_rate, batch_size, use_regularization):\n",
    "    model = Sequential()\n",
    "\n",
    "    if use_regularization:\n",
    "        # L2 regularization with weight decay of 0.01\n",
    "        from tensorflow.keras.regularizers import l2\n",
    "\n",
    "        model.add(LSTM(neurons, activation='relu', kernel_regularizer=l2(0.01), input_shape=(batch_size, n_features), kernel_initializer=HeNormal()))\n",
    "    else:\n",
    "        model.add(LSTM(neurons, activation='relu', input_shape=(batch_size, n_features),\n",
    "                      kernel_initializer=HeNormal()))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(32, activation=\"relu\", kernel_initializer=HeNormal()))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1, activation=\"sigmoid\", kernel_initializer=HeNormal()))\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "param_grid = {\n",
    "  'neurons': [32, 64, 128],\n",
    "  'dropout_rate': [0.2, 0.3, 0.4],\n",
    "  'learning_rate': [0.01, 0.001],\n",
    "  'batch_size': [32],\n",
    "  'use_regularization': [True, False]\n",
    "}\n",
    "\n",
    "X = df[['related', 'source', 'positive', 'negative', 'neutral']].values  # Assuming 'related' and 'source' are your features\n",
    "y = df[[\"market_change\"]].values  # Assuming 'positive', 'negative', 'neutral' are your labels\n",
    "\n",
    "# Reshape the data for LSTM input (assuming your data has a time dimension)\n",
    "n_features = X.shape[1]\n",
    "\n",
    "# Create the grid search object\n",
    "grid_search = GridSearchCV(estimator=KerasClassifier(build_fn=create_model, epochs=10),\n",
    "                           param_grid=param_grid, cv=3, verbose=2)\n",
    "\n",
    "X_train_reshaped, y_train_reshaped, X_val_reshaped, y_val_reshaped, X_test_reshaped, y_test_reshaped = \\\n",
    "reshape_data(df, seq_length_news=32)\n",
    "\n",
    "\n",
    "grid_search.fit(X_train_reshaped, y_train_reshaped, \n",
    "                validation_data=(X_val_reshaped, y_val_reshaped), callbacks=[checkpoint])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab0e107",
   "metadata": {},
   "source": [
    "The model training during the regular training and the grid search platoed and wouldn't improve the validation accuracy (it did overfit after a while)\n",
    "\n",
    "There are several options I can do to try to fine tune and improve the results by some quantity (the model won't improve to 90% but can improve by some percents) - \n",
    "1. feature engeneering - try to add one hot encoding, remove some of the columns etc...\n",
    "2. try to make the model more complex - add layers and neurons\n",
    "3. change batch size\n",
    "4. change the loss function\n",
    "5. convert the model to use bert and the BOW\n",
    "6. add more data to the dataset - for example the previous market_change, or more news headlines that are not necessarily related to the stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b95d89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7bc9563c",
   "metadata": {},
   "source": [
    "## DNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd071cc",
   "metadata": {},
   "source": [
    "Trying more calssical DNN composed of only Dense layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211d3ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "        keras.layers.Dense(100, activation=\"relu\", kernel_initializer=glorot_uniform()),\n",
    "        keras.layers.Dense(100, activation=\"relu\", kernel_initializer=glorot_uniform()),\n",
    "        keras.layers.Dense(50, activation=\"relu\", kernel_initializer=glorot_uniform()),\n",
    "        keras.layers.Dense(2, activation=\"softmax\", kernel_initializer=glorot_uniform())\n",
    "])\n",
    "\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(learning_rate=0.01),\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "callback_early_stopping = keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "X_train.fillna(0, inplace=True)\n",
    "X_valid.fillna(0, inplace=True)\n",
    "\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=1000, \n",
    "                    validation_data=(X_valid, y_valid), callbacks=[callback_early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0a89f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a40d0b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e95e737",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddd606d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
